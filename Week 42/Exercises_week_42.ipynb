{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be117070",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek42.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 42 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7356c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercises week 42\n",
    "**October 9-13, 2023**\n",
    "\n",
    "Date: **Deadline is Sunday October 22 at midnight**\n",
    "\n",
    "You can hand in the exercises from week 41 and week 42 as one exercise and get a total score of two additional points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa378ef2",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The aim of the exercises this week is to get started with implementing\n",
    "gradient methods of relevance for project 2. The exercise this week is a simple\n",
    "continuation from the  previous week with the addition of automatic differentation.\n",
    "Everything you develop here will be used in project 2. \n",
    "\n",
    "In order to get started, we will now replace in our standard ordinary\n",
    "least squares (OLS) and Ridge regression codes (from project 1) the\n",
    "matrix inversion algorithm with our own gradient descent (GD) and SGD\n",
    "codes.  You can use the Franke function or the terrain data from\n",
    "project 1. **However, we recommend using a simpler function like**\n",
    "$f(x)=a_0+a_1x+a_2x^2$ or higher-order one-dimensional polynomials.\n",
    "You can obviously test your final codes against for example the Franke\n",
    "function. Automatic differentiation will be discussed next week.\n",
    "\n",
    "You should include in your analysis of the GD and SGD codes the following elements\n",
    "1. A plain gradient descent with a fixed learning rate (you will need to tune it) using automatic differentiation. Compare this with the analytical expression of the gradients you obtained last week. Feel free to use **Autograd** as Python package or **JAX**. You can use the examples form last week.\n",
    "\n",
    "2. Add momentum to the plain GD code and compare convergence with a fixed learning rate (you may need to tune the learning rate). Compare this with the analytical expression of the gradients you obtained last week.\n",
    "\n",
    "3. Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. Use a tunable learning rate as discussed in the lectures from week 39. Discuss the results as functions of the various parameters (size of batches, number of epochs etc)\n",
    "\n",
    "4. Implement the Adagrad method in order to tune the learning rate. Do this with and without momentum for plain gradient descent and SGD using automatic differentiation..\n",
    "\n",
    "5. Add RMSprop and Adam to your library of methods for tuning the learning rate. Again using automatic differentiation.\n",
    "\n",
    "The lecture notes from weeks 39 and 40 contain more information and code examples. Feel free to use these examples.\n",
    "\n",
    "We recommend reading chapter 8 on optimization from the textbook of [Goodfellow, Bengio and Courville](https://www.deeplearningbook.org/). This chapter contains many useful insights and discussions on the optimization part of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c41818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\" Data \"\"\"\n",
    "def FrankeFunction(x,y, noice, alpha, seed):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    if noice:\n",
    "        np.random.seed(seed)\n",
    "        return term1 + term2 + term3 + term4 + alpha*np.random.normal(0, 1, x.shape)\n",
    "    else:\n",
    "        return term1 + term2 + term3 + term4\n",
    "\n",
    "def data_FF(noise=True, step_size=0.05, alpha=0.05, reshape=True):\n",
    "    x = np.arange(0, 1, step_size)\n",
    "    y = np.arange(0, 1, step_size)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    if reshape:\n",
    "        x = X.flatten().reshape(-1, 1)\n",
    "        y = Y.flatten().reshape(-1, 1)\n",
    "        Z = FrankeFunction(X, Y, noise, alpha, seed=3155)\n",
    "        z = Z.flatten().reshape(-1, 1)\n",
    "        return x, y, z\n",
    "    if not reshape:\n",
    "        Z = FrankeFunction(X, Y, noise, alpha, seed=3155)\n",
    "        return X, Y, Z\n",
    "\n",
    "\n",
    "def y_func(x, exact_theta):\n",
    "    y = 0\n",
    "    for i, theta in enumerate(exact_theta):\n",
    "        y += theta*x**i\n",
    "    return y\n",
    "\n",
    "def polynomial(coeff, n, noise, alpha, seed):\n",
    "    X = np.linspace(0, 1, n).reshape(-1, 1)    \n",
    "    Y_true = y_func(X, coeff)\n",
    "    if noise:\n",
    "        np.random.seed(seed)\n",
    "        Y_noise = (Y_true + alpha*np.random.normal(0, 1, X.shape))\n",
    "    else:\n",
    "        Y_noise = Y_true\n",
    "    return X, Y_noise, Y_true\n",
    "\n",
    "def xor():\n",
    "    X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2))\n",
    "    Y = np.reshape([[0], [1], [1], [0]], (4, 1))\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "def scaling(X):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X[:,0] = 1\n",
    "    return X\n",
    "\n",
    "def designMatrix_1D(x, polygrad):\n",
    "    n = len(x)\n",
    "    X = np.ones((n,polygrad))\n",
    "    for i in range(1,polygrad):\n",
    "        X[:,i] = (x**i).ravel()\n",
    "    return X\n",
    "\n",
    "def learning_schedule(epoch, init_LR, decay):\n",
    "    return init_LR * 1/(1 + decay*epoch)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Task a) \"\"\"\n",
    "def OLS(X_train, X_test, y):\n",
    "    \"\"\" OLS \"\"\"\n",
    "    XT_X = X_train.T @ X_train\n",
    "    theta_linreg = np.linalg.pinv(XT_X) @ (X_train.T @ y)\n",
    "    y_predict_OLS = X_test @ theta_linreg\n",
    "    return y_predict_OLS, theta_linreg\n",
    "\n",
    "def cost_theta(theta, X, y, lmb):\n",
    "    n = X.shape[0]\n",
    "    return (1.0/n)*np.sum((y - (X @ theta))**2) + (lmb/2) * (theta.T@theta)\n",
    "\n",
    "def cost_theta_diff(theta, X, y, lmb):\n",
    "    n = X.shape[0]\n",
    "    return lmb*theta - 2*X.T @(y - X@theta)/n\n",
    "\n",
    "def auto_gradient(theta, X, y, lmb):\n",
    "    gradient = grad(cost_theta)\n",
    "    return gradient(theta, X, y, lmb)\n",
    "\n",
    "\n",
    "\"\"\" NN \"\"\"\n",
    "def cost_w_b(w, b, X, y, lmb):\n",
    "    n = X.shape[0]\n",
    "    return (1.0/n)*np.sum((y - (X @ w + b))**2) + (lmb/2)*(w.T@w)\n",
    "\n",
    "def cost_w_b_diff(w, b, X, y, lmb):\n",
    "    n = X.shape[0]\n",
    "    grad_w = lmb*w + np.sum(2*X*(b + w*X - y))/n\n",
    "    grad_b = (2/n)*np.sum(b + w*X - y)\n",
    "    return grad_w, grad_b\n",
    "\n",
    "def cost(z, z_tilde, lmb, w):\n",
    "    return mean_squared_error(z, z_tilde) + (lmb/2)*(w.T@w)\n",
    "\n",
    "def cost_diff(z, z_tilde, lmb, w):\n",
    "    return 2 * (z_tilde - z) / np.size(z_tilde) # + lmb*w\n",
    "\n",
    "def MSE(z, z_tilde):\n",
    "    return mean_squared_error(z, z_tilde)\n",
    "\n",
    "def MSE_diff(z, z_tilde):\n",
    "    return 2 * (z_tilde - z) / np.size(z_tilde)\n",
    "\n",
    "def auto_gradient_NN(w, b, X, y, lmb):\n",
    "    w_gradient = grad(cost_w_b,0)\n",
    "    b_gradient = grad(cost_w_b,1)\n",
    "    return w_gradient(w, b, X, y, lmb), b_gradient(w, b, X, y, lmb)\n",
    "\n",
    "def r2(z, z_tilde):\n",
    "    return r2_score(z, z_tilde)\n",
    "\n",
    "# def Cost_GD(w, b, X, y, lmb):\n",
    "#     n = X.shape[0]\n",
    "#     return (1.0/n)*np.sum((y - (X @ w + b))**2) + (lmb/2) * w**2\n",
    "\n",
    "# def Cost_GD_diff(w, b, X, y, lmb):\n",
    "#     n = X.shape[0]\n",
    "#     grad_w = lmb*w + (2*X*(b + w*X - y))/n # (2/n)*X*(b + w*X - y) # 2.0/n * X.T @ ((X @ w + b) - y)\n",
    "#     grad_b = (2/n)*(b + w*X - y)\n",
    "#     return np.mean(grad_w, axis=1), np.mean(grad_b, axis=1)\n",
    "\n",
    "\n",
    "\"\"\" Logistic Regression \"\"\"\n",
    "\n",
    "def accuracy(y_pred,y):\n",
    "    \"\"\"Logistic:\"\"\"\n",
    "    accuracy = np.mean(y_pred.flatten()==y.flatten())\n",
    "    return accuracy\n",
    "\n",
    "def cross_entropy_cost(y_train, output, lmb, weights):\n",
    "    \"\"\"the binary cross entropy cost function\n",
    "    lmb is L2 reg. w is for the regularization of NN\"\"\"\n",
    "    eps =1e-15 # small epsilon for numeric stability.\n",
    "    p = output #prediction.\n",
    "    y = y_train\n",
    "    w= weights\n",
    "    cost = - np.sum(y*np.log(p+eps) + (1-y)*np.log(1-p+eps))+ (lmb/2)* w.T @ w\n",
    "    cost = cost/y.shape[0]\n",
    "    return cost\n",
    "\n",
    "def diff_cross_entropy_cost(y_train, output, lmb, weights):\n",
    "    \"\"\"The derivative of the cost function for cross entropy NN classifier.\n",
    "    Note that L2 regularisation happens in the layer.py backprop\"\"\"\n",
    "    p = output\n",
    "    y = y_train\n",
    "    w = weights\n",
    "\n",
    "    grad_cross_entropi =  (p - y) #Simplified for first layer backwards.\n",
    "    mean = np.mean(grad_cross_entropi,axis =1)\n",
    "    gradient_CE = np.expand_dims(mean, axis=1)\n",
    "    return gradient_CE\n",
    "\n",
    "def grad_cost_func_logregression(beta, X, y, lmb):\n",
    "    \"\"\"The derivative of the cost function for logistic regression\"\"\"\n",
    "    p = 1/(1+np.exp(-X@beta))\n",
    "    grad_cross_entropi = - X.T @ (y - p) +lmb*beta\n",
    "    mean = np.mean(grad_cross_entropi,axis =1)\n",
    "    gradient_CE = np.expand_dims(mean, axis=1)\n",
    "    return gradient_CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e26da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"A super class for three optimizers.\"\"\"\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "        self.delta = 1e-7 #to avoid division by zero.\n",
    "\n",
    "    def __call__(self,gradients):\n",
    "        raise TypeError(\"You need to specify which Optimizer\")\n",
    "\n",
    "class Adagrad(Optimizer):\n",
    "    def __call__(self,gradients, Giter):\n",
    "        Giter += gradients @ gradients.T\n",
    "        self.Ginverse = np.c_[self.eta/(self.delta + np.sqrt(np.diagonal(Giter)))]\n",
    "        return np.multiply(self.Ginverse,gradients)\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "    def __call__(self,gradients, Giter):\n",
    "        beta = 0.90 #Ref Geron boka.\n",
    "        Previous = Giter.copy() #stores the current Giter.\n",
    "        Giter += gradients @ gradients.T\n",
    "        Giter = (beta*Previous + (1 - beta)*Giter)\n",
    "        self.Ginverse = np.c_[self.eta/(self.delta + np.sqrt(np.diagonal(Giter)))]\n",
    "        return np.multiply(self.Ginverse,gradients)\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc rand\n",
    "    Algoritm 8.7 Adam in Chapter 8 of Ian Goodfellow\"\"\"\n",
    "    def __init__(self,eta):\n",
    "        super().__init__(eta) # Optimizer stores these.\n",
    "        self.m = 0\n",
    "        self.s = 0\n",
    "        self.t = 1\n",
    "        self.beta_1 = 0.90 #Ref Geron and Goodfellow bøkene.\n",
    "        self.beta_2 = 0.999 #Ref Geron and Goodfellow bøkene.\n",
    "\n",
    "    def __call__(self, gradients):\n",
    "\n",
    "        #Update of 1st and 2nd moment:\n",
    "        m = (self.beta_1*self.m + (1 - self.beta_1)*gradients)\n",
    "        s = (self.beta_2*self.s + (1 - self.beta_2)*gradients**2)\n",
    "\n",
    "        #Bias correction:\n",
    "        self.mHat = m/(1 - self.beta_1**self.t) #med tidsteg t.\n",
    "        self.sHat = s/(1 - self.beta_2**self.t)\n",
    "\n",
    "        #Compute update:\n",
    "        self.Ginverse = self.eta/(self.delta + np.sqrt(self.sHat))\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.t += 1\n",
    "        \n",
    "        return np.multiply(self.Ginverse,self.mHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04f23cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GD(X_train, X_test, y_train, y_test, Gradient_method, Optimizer_method, Niterations, init_LR, decay, momentum, seed, lmb):\n",
    "\n",
    "    \"\"\" Gradient Decent \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    theta = np.random.randn(np.shape(X_train)[1],1) # Initial thetas/betas\n",
    "\n",
    "    change = 0.0\n",
    "    mse_test = np.zeros(Niterations)\n",
    "    mse_train = np.zeros(Niterations)\n",
    "    \n",
    "    \"\"\" Optimizer method \"\"\"\n",
    "    if Optimizer_method == 'Adagrad':\n",
    "        optim = Adagrad(init_LR)\n",
    "    if Optimizer_method == 'RMSprop':\n",
    "        optim = RMSprop(init_LR)\n",
    "    if Optimizer_method == 'Adam':\n",
    "        optim = Adam(init_LR)\n",
    "\n",
    "    # Gradient decent:\n",
    "    Giter = np.zeros(shape=(X_train.shape[1],X_train.shape[1]))\n",
    "    for i in range(Niterations):\n",
    "\n",
    "        \"\"\" Gradient method \"\"\"\n",
    "        if Gradient_method == 'auto':  \n",
    "            gradients =  auto_gradient(theta, X_train, y_train, lmb) # Autograd\n",
    "        if Gradient_method == 'anal':\n",
    "            gradients = cost_theta_diff(theta, X_train, y_train, lmb) # Analytical\n",
    "\n",
    "        \"\"\" Optimizer method \"\"\"\n",
    "        if Optimizer_method == 'Adagrad':\n",
    "            update = optim(gradients, Giter)#uses class\n",
    "            theta -= update\n",
    "\n",
    "        if Optimizer_method == 'RMSprop':\n",
    "            update = optim(gradients, Giter)#uses class\n",
    "            theta -= update\n",
    "\n",
    "        if Optimizer_method == 'Adam':\n",
    "            update = optim(gradients)\n",
    "            theta -= update\n",
    "\n",
    "        if Optimizer_method == 'momentum':\n",
    "            eta = learning_schedule(i, init_LR, decay) # LR\n",
    "            update = eta * gradients + momentum * change # Update to the thetas\n",
    "\n",
    "            theta -= update\n",
    "            change = update # Update the amount the momentum gets added\n",
    "\n",
    "        y_predict_GD_test = X_test @ theta\n",
    "        mse_test[i] = cost(y_test, y_predict_GD_test, lmb, theta)\n",
    "\n",
    "        y_predict_GD_train = X_train @ theta\n",
    "        mse_train[i] = cost(y_train, y_predict_GD_train, lmb, theta)\n",
    "\n",
    "    y_predict_GD = X_test @ theta\n",
    "\n",
    "    return y_predict_GD, theta, mse_test, mse_train\n",
    "\n",
    "def SGD(X_train, X_test, y_train, y_test, Optimizer_method, Gradient_method, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, lmb):\n",
    "    \"\"\" Stochastic Gradient Decent \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    theta = np.random.randn(np.shape(X_train)[1],1) # Initial thetas/betas\n",
    "\n",
    "    mse_test = np.zeros(n_epochs*n_minibatches)\n",
    "    mse_train = np.zeros(n_epochs*n_minibatches)\n",
    "\n",
    "    count = 0\n",
    "    change = 0.0\n",
    "\n",
    "    \"\"\" Optimizer method \"\"\"\n",
    "    if Optimizer_method == 'Adagrad':\n",
    "        optim = Adagrad(init_LR)\n",
    "    if Optimizer_method == 'RMSprop':\n",
    "        optim = RMSprop(init_LR)\n",
    "    if Optimizer_method == 'Adam':\n",
    "        optim = Adam(init_LR)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        Giter = np.zeros(shape=(X_train.shape[1],X_train.shape[1]))\n",
    "        for batch in range(n_minibatches):\n",
    "\n",
    "            random_index = minibatch_size*np.random.randint(n_minibatches)\n",
    "            X_batch = X_train[random_index:random_index+minibatch_size]\n",
    "            y_batch = y_train[random_index:random_index+minibatch_size]\n",
    "\n",
    "\n",
    "            \"\"\" Gradient method \"\"\"\n",
    "            if Gradient_method == 'auto':  \n",
    "                gradients =  auto_gradient(theta, X_batch, y_batch, lmb) # Autograd\n",
    "            if Gradient_method == 'anal':\n",
    "                gradients = cost_theta_diff(theta, X_batch, y_batch, lmb) # Analytical\n",
    "\n",
    "            \"\"\" Optimizer method \"\"\"\n",
    "            if Optimizer_method == 'Adagrad':\n",
    "                update = optim(gradients, Giter)#uses class\n",
    "                theta -= update\n",
    "\n",
    "            if Optimizer_method == 'RMSprop':\n",
    "                update = optim(gradients, Giter)#uses class\n",
    "                theta -= update\n",
    "\n",
    "            if Optimizer_method == 'Adam':\n",
    "                update = optim(gradients)\n",
    "                theta -= update\n",
    "\n",
    "            if Optimizer_method == 'momentum':\n",
    "                eta = learning_schedule(epoch, init_LR, decay) # LR\n",
    "                update = eta * gradients + momentum * change # Update to the thetas\n",
    "                theta -= update\n",
    "                change = update\n",
    "\n",
    "\n",
    "            y_predict_SGD_test = X_test @ theta\n",
    "            mse_test[count] = cost(y_test, y_predict_SGD_test, lmb, theta)\n",
    "\n",
    "            y_predict_SGD_train = X_train @ theta\n",
    "            mse_train[count] = cost(y_train, y_predict_SGD_train, lmb, theta)\n",
    "            count += 1\n",
    "\n",
    "    y_predict_GD = X_test @ theta\n",
    "    return y_predict_GD, theta, mse_test, mse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f4d4d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tz/r4b14w1928x5k5795yw_9s3h0000gn/T/ipykernel_6922/1283451058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\"\"\" Plots: \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m \u001b[0mbest_poly_deg_GD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lambda_GD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_lambda_DG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_polydeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0mbest_poly_deg_SGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lambda_SGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_lambda_SDG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_polydeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0mbest_poly_deg_GD\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tz/r4b14w1928x5k5795yw_9s3h0000gn/T/ipykernel_6922/1283451058.py\u001b[0m in \u001b[0;36mfind_lambda_DG\u001b[0;34m(x_train, x_test, y_train, y_test, x, y_true, G_M, lambda_min, lambda_max, nlambdas, max_polydeg, plot, n_epochs, seed)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0my_predict_GD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cost_GD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cost_GD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mcost_lambda_degree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_cost_GD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tz/r4b14w1928x5k5795yw_9s3h0000gn/T/ipykernel_6922/2693636893.py\u001b[0m in \u001b[0;36mGD\u001b[0;34m(X_train, X_test, y_train, y_test, Gradient_method, Optimizer_method, Niterations, init_LR, decay, momentum, seed, lmb)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mmse_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict_GD_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0my_predict_GD_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mmse_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict_GD_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" PLOTS: \"\"\"\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"ComputerModern\"]})\n",
    "\n",
    "\"\"\" (1/8) Heatmap of GD to find best lambda and model complexity \"\"\"\n",
    "def find_lambda_DG(x_train, x_test, y_train, y_test, x, y_true, G_M, lambda_min, lambda_max, nlambdas, max_polydeg, plot, n_epochs, seed):\n",
    "\n",
    "    lambdas = np.logspace(lambda_min, lambda_max, nlambdas)\n",
    "    polydeg = np.arange(max_polydeg)\n",
    "    cost_lambda_degree = np.empty((nlambdas, max_polydeg))\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.0                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.0                          # Momentum value for GD.\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[3] # Choose the optimization method\n",
    "\n",
    "    for d_idx, deg in enumerate(polydeg):\n",
    "\n",
    "        X_train = designMatrix_1D(x_train, deg + 1)\n",
    "        X_test = designMatrix_1D(x_test, deg + 1)\n",
    "\n",
    "        for l_idx, lmb in enumerate(lambdas):\n",
    "            y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train, X_test, y_train, y_test, G_M, O_M, n_epochs, init_LR, decay, momentum, seed, lmb)\n",
    "            cost_lambda_degree[l_idx, d_idx] = test_cost_GD[-1]\n",
    "    \n",
    "    index = np.argwhere(cost_lambda_degree == np.min(cost_lambda_degree))\n",
    "    best_poly_deg_cost = polydeg[index[0,1]]\n",
    "    best_lambda_cost = lambdas[index[0,0]]\n",
    "\n",
    "    print(f'The lowest cost with GD was achieved at polynomial degree = {best_poly_deg_cost}, and with lambda = {best_lambda_cost}.')\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(14,8))\n",
    "        plt.rcParams.update({'font.size': 26})\n",
    "        sns.heatmap(cost_lambda_degree[:,1:], cmap=\"RdYlGn_r\", \n",
    "        annot=True, annot_kws={\"size\": 20},\n",
    "        fmt=\"1.4f\", linewidths=1, linecolor=(30/255,30/255,30/255,1),\n",
    "        cbar_kws={\"orientation\": \"horizontal\", \"shrink\":0.8, \"aspect\":40, \"label\":r\"Cost\", \"pad\":0.05})\n",
    "        x_idx = np.arange(max_polydeg-1) + 0.5\n",
    "        y_idx = np.arange(nlambdas) + 0.5\n",
    "        ax.set_xticks(x_idx, [deg for deg in polydeg[1:]], fontsize='medium')\n",
    "        ax.set_yticks(y_idx, [float(f'{lam:1.1E}') for lam in lambdas], rotation=0, fontsize='medium')\n",
    "        ax.set_xlabel(r\"Polynomial degree\", labelpad=10, fontsize='medium')\n",
    "        ax.set_ylabel(r'$\\log_{10} \\lambda$', labelpad=10, fontsize='medium')\n",
    "        ax.set_title(r'\\bf{Cost Heatmap for plain GD}', pad=15)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('cost_heatmap_plain_GD_LR_0_1.png', dpi=150)\n",
    "        plt.clf()\n",
    "\n",
    "        plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "        axisbelow=True, grid=True)\n",
    "        plt.rc('grid', color='w', linestyle='solid')\n",
    "        plt.rc('lines', linewidth=2)\n",
    "\n",
    "        X_train = designMatrix_1D(x_train, best_poly_deg_cost + 1)\n",
    "        X_test = designMatrix_1D(x_test, best_poly_deg_cost + 1)\n",
    "\n",
    "        y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train, X_test, y_train, y_test, G_M, O_M, n_epochs, init_LR, decay, momentum, seed, lmb)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16,9))\n",
    "        fig.subplots_adjust(bottom=0.22)\n",
    "        \"\"\" Regression line plot \"\"\"\n",
    "        ax.scatter(x_test, y_predict_GD, c='limegreen', s=5, label=r'GD')\n",
    "        # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "        ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "        ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "        ax.set_title(r'\\bf{Regression line plot for plain GD}', pad=15)\n",
    "        ax.set_xlabel(r'$x$', labelpad=10)\n",
    "        ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "        ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "        string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}'\n",
    "        plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "        plt.savefig('regression_line_plot_plain_GD.png', dpi=150)\n",
    "        plt.clf()\n",
    "\n",
    "        iters = np.arange(n_epochs)\n",
    "        fig, ax = plt.subplots(figsize=(16,9))\n",
    "        fig.subplots_adjust(bottom=0.22)\n",
    "        ax.plot(iters, train_cost_GD, color='crimson', zorder=100, lw=2, label=r\"Train Cost for GD\") #zorder=0,\n",
    "        ax.plot(iters, test_cost_GD, color='royalblue', lw=2, label=r\"Test Cost for GD\") #zorder=0,\n",
    "        # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "        ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "        ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "        ax.set_title(r\"\\bf{Cost as function of iterations for plain GD}\", pad=15)\n",
    "        ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "        ax.set_yscale('log')\n",
    "        string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}'\n",
    "        plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "        plt.savefig('cost_plot_plain_GD.png', dpi=150)\n",
    "        plt.clf()\n",
    "\n",
    "    return best_poly_deg_cost, best_lambda_cost\n",
    "\n",
    "\"\"\" (2/8) Heatmap of SGD to find best lambda and model complexity \"\"\"\n",
    "def find_lambda_SDG(x_train, x_test, y_train, y_test, x, y_true, G_M, lambda_min, lambda_max, nlambdas, max_polydeg, plot, n_epochs, seed):\n",
    "\n",
    "    lambdas = np.logspace(lambda_min, lambda_max, nlambdas)\n",
    "    polydeg = np.arange(max_polydeg)\n",
    "    cost_lambda_degree = np.empty((nlambdas, max_polydeg))\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.0                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.0                          # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//20\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[3] # Choose the optimization method\n",
    "\n",
    "    for d_idx, deg in enumerate(polydeg):\n",
    "\n",
    "        X_train = designMatrix_1D(x_train, deg + 1)\n",
    "        X_test = designMatrix_1D(x_test, deg + 1)\n",
    "\n",
    "        for l_idx, lmb in enumerate(lambdas):\n",
    "            y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train, X_test, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, lmb)\n",
    "            cost_lambda_degree[l_idx, d_idx] = test_cost_SGD[-1]\n",
    "    \n",
    "    index = np.argwhere(cost_lambda_degree == np.min(cost_lambda_degree))\n",
    "    best_poly_deg_cost = polydeg[index[0,1]]\n",
    "    best_lambda_cost = lambdas[index[0,0]]\n",
    "\n",
    "    print(f'The lowest cost with SGD was achieved at polynomial degree = {best_poly_deg_cost}, and with lambda = {best_lambda_cost}.')\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(14,8))\n",
    "        plt.rcParams.update({'font.size': 26})\n",
    "        sns.heatmap(cost_lambda_degree[:,1:], cmap=\"RdYlGn_r\", \n",
    "        annot=True, annot_kws={\"size\": 20},\n",
    "        fmt=\"1.4f\", linewidths=1, linecolor=(30/255,30/255,30/255,1),\n",
    "        cbar_kws={\"orientation\": \"horizontal\", \"shrink\":0.8, \"aspect\":40, \"label\":r\"Cost\", \"pad\":0.05})\n",
    "        x_idx = np.arange(max_polydeg-1) + 0.5\n",
    "        y_idx = np.arange(nlambdas) + 0.5\n",
    "        ax.set_xticks(x_idx, [deg for deg in polydeg[1:]], fontsize='medium')\n",
    "        ax.set_yticks(y_idx, [float(f'{lam:1.1E}') for lam in lambdas], rotation=0, fontsize='medium')\n",
    "        ax.set_xlabel(r\"Polynomial degree\", labelpad=10, fontsize='medium')\n",
    "        ax.set_ylabel(r'$\\log_{10} \\lambda$', labelpad=10, fontsize='medium')\n",
    "        ax.set_title(r'\\bf{Cost Heatmap for plain SGD}', pad=15)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.xaxis.set_label_position('top')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('cost_heatmap_plain_SGD_LR_0_1.png', dpi=150)\n",
    "        plt.clf()\n",
    "\n",
    "        plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "        axisbelow=True, grid=True)\n",
    "        plt.rc('grid', color='w', linestyle='solid')\n",
    "        plt.rc('lines', linewidth=2)\n",
    "\n",
    "        X_train = designMatrix_1D(x_train, best_poly_deg_cost + 1)\n",
    "        X_test = designMatrix_1D(x_test, best_poly_deg_cost + 1)\n",
    "\n",
    "        y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train, X_test, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_cost)\n",
    "        \n",
    "        \"\"\" Regression line plot \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(16,9))\n",
    "        fig.subplots_adjust(bottom=0.22)\n",
    "        ax.scatter(x_test, y_predict_SGD, c='limegreen', s=5, label=r'SGD')\n",
    "        # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "        ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "        ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "        ax.set_title(r'\\bf{Regression line plot for plain SGD}', pad=15)\n",
    "        ax.set_xlabel(r'$x$', labelpad=10)\n",
    "        ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "        ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "        string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "        plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "        plt.savefig('regression_line_plot_plain_SGD.png', dpi=150)\n",
    "        plt.clf()\n",
    "\n",
    "        iters = np.arange(n_epochs*n_minibatches)\n",
    "        fig, ax = plt.subplots(figsize=(16,9))\n",
    "        fig.subplots_adjust(bottom=0.22)\n",
    "        ax.plot(iters, train_cost_SGD, color='crimson', zorder=100, lw=2, label=r\"Train Cost for SGD\") #zorder=0,\n",
    "        ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Test Cost for SGD\") #zorder=0,\n",
    "        # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "        ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "        ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "        ax.set_title(r\"\\bf{Cost as function of iterations for plain SGD}\", pad=15)\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "        string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "        plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "        plt.savefig('cost_plot_plain_SGD.png', dpi=150)\n",
    "        plt.clf()\n",
    "\n",
    "    return best_poly_deg_cost, best_lambda_cost\n",
    "\n",
    "\"\"\" Func to find best minibatch size \"\"\"\n",
    "def find_minibatch_size_SDG(x_train, x_test, y_train, y_test, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed):\n",
    "\n",
    "    minibatchs = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "    X_train = designMatrix_1D(x_train, best_poly_deg_SGD)\n",
    "    X_test = designMatrix_1D(x_test, best_poly_deg_SGD)\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.0                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.0                          # Momentum value for GD.\n",
    "\n",
    "    cost_minibatch = np.zeros(len(minibatchs))\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[3] # Choose the optimization method\n",
    "\n",
    "    for mb_idx, size in enumerate(minibatchs):\n",
    "        minibatch_size = np.shape(x_train)[0]//size\n",
    "        n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "        y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train, X_test, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "        cost_minibatch[mb_idx] = test_cost_SGD[-1]\n",
    "\n",
    "    index = np.argwhere(cost_minibatch == np.min(cost_minibatch))\n",
    "    best_minibatch_size = minibatchs[index[0,0]]\n",
    "\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    print(f'The lowest cost with SGD was achieved with minibatch size = {minibatch_size}. Thus, {n_minibatches} minibatches.')\n",
    "    return best_minibatch_size\n",
    "\n",
    "\"\"\" (3/8) cost-plot for GD and SGD with a fixed learning rate using the chosen lambda \"\"\"\n",
    "def fixed_LR(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.0                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.0                          # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[3] # Choose the optimization method\n",
    "\n",
    "    X_train_GD = designMatrix_1D(x_train, best_poly_deg_GD) # Train design matrix for GD \n",
    "    X_test_GD = designMatrix_1D(x_test, best_poly_deg_GD) # Test design matrix for GD\n",
    "\n",
    "    X_train_SGD = designMatrix_1D(x_train, best_poly_deg_SGD) # Train design matrix for SGD\n",
    "    X_test_SGD = designMatrix_1D(x_test, best_poly_deg_SGD) # Test design matrix for SGD\n",
    "\n",
    "    y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "    y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.plot(iters, test_cost_GD, color='crimson', lw=2, zorder=100, label=r\"Cost for the test data - GD\") #zorder=0,\n",
    "    ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Cost for the test data - SGD\") #zorder=0,\n",
    "    # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for fixed LR}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_fixed_LR.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.scatter(x_test, y_predict_SGD, c='limegreen', s=5, label='SGD')\n",
    "    ax.scatter(x_test, y_predict_GD, c='crimson', s=5,label='GD')\n",
    "    # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for fixed LR}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_fixed_LR.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "\"\"\" (4/8) cost-plot for GD and SGD with a fixed learning rate and momentum using the chosen lambda \"\"\"\n",
    "def fixed_LR_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.0                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.9                          # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[3] # Choose the optimization method\n",
    "\n",
    "    X_train_GD = designMatrix_1D(x_train, best_poly_deg_GD) # Train design matrix for GD \n",
    "    X_test_GD = designMatrix_1D(x_test, best_poly_deg_GD) # Test design matrix for GD\n",
    "\n",
    "    X_train_SGD = designMatrix_1D(x_train, best_poly_deg_SGD) # Train design matrix for SGD\n",
    "    X_test_SGD = designMatrix_1D(x_test, best_poly_deg_SGD) # Test design matrix for SGD\n",
    "\n",
    "    y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "    y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.plot(iters, test_cost_GD, color='crimson', lw=2, zorder=100, label=r\"Cost for the test data - GD\") #zorder=0,\n",
    "    ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Cost for the test data - SGD\") #zorder=0,\n",
    "    # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for fixed LR and momentum}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_fixed_LR_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.scatter(x_test, y_predict_SGD, c='limegreen', s=5, label='SGD')\n",
    "    ax.scatter(x_test, y_predict_GD, c='crimson', s=5,label='GD')\n",
    "    # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for fixed LR and momentum}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_fixed_LR_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "\"\"\" (5/8) cost-plot for GD and SGD with an adaptive learning rate and momentum using the chosen lambda \"\"\"\n",
    "def adaptive_LR_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.01                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.9                        # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[3] # Choose the optimization method\n",
    "\n",
    "    X_train_GD = designMatrix_1D(x_train, best_poly_deg_GD) # Train design matrix for GD \n",
    "    X_test_GD = designMatrix_1D(x_test, best_poly_deg_GD) # Test design matrix for GD\n",
    "\n",
    "    X_train_SGD = designMatrix_1D(x_train, best_poly_deg_SGD) # Train design matrix for SGD\n",
    "    X_test_SGD = designMatrix_1D(x_test, best_poly_deg_SGD) # Test design matrix for SGD\n",
    "\n",
    "    y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "    y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Cost for the test data - SGD\") #zorder=0,\n",
    "    ax.plot(iters, test_cost_GD, color='crimson', lw=2, zorder=100, label=r\"Cost for the test data - GD\") #zorder=0,\n",
    "    # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for adaptive LR and momentum}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_adaptive_LR_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.scatter(x_test, y_predict_SGD, c='limegreen', s=5, label='SGD')\n",
    "    ax.scatter(x_test, y_predict_GD, c='crimson', s=5,label='GD')\n",
    "    # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for adaptive LR and momentum}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_adaptive_LR_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "\"\"\" (6/8) cost-plot for GD and SGD with Adagrad with momentum using the chosen lambda \"\"\"\n",
    "def Adagrad_w_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.01                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.9                        # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[0] # Choose the optimization method\n",
    "\n",
    "    X_train_GD = designMatrix_1D(x_train, best_poly_deg_GD) # Train design matrix for GD \n",
    "    X_test_GD = designMatrix_1D(x_test, best_poly_deg_GD) # Test design matrix for GD\n",
    "\n",
    "    X_train_SGD = designMatrix_1D(x_train, best_poly_deg_SGD) # Train design matrix for SGD\n",
    "    X_test_SGD = designMatrix_1D(x_test, best_poly_deg_SGD) # Test design matrix for SGD\n",
    "\n",
    "    y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "    y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Cost for the test data - SGD\") #zorder=0,\n",
    "    ax.plot(iters, test_cost_GD, color='crimson', lw=2, zorder=100, label=r\"Cost for the test data - GD\") #zorder=0,\n",
    "    # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for Adagrad w/ momentum}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_Adagrad_w_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.scatter(x_test, y_predict_SGD, c='limegreen', s=5, label='SGD')\n",
    "    ax.scatter(x_test, y_predict_GD, c='crimson', s=5,label='GD')\n",
    "    # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for Adagrad w/ momentum}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_Adagrad_w_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "  \n",
    "\"\"\" (7/8) cost-plot for GD and SGD with Adagrad without momentum using the chosen lambda \"\"\"\n",
    "def Adagrad_w_o_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.01                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.0                        # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    \"\"\" Optimization method \"\"\"\n",
    "    # To get plain GD without any optimization choose 'momentum' with momentum value of 0\n",
    "    Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    O_M = Optimizer_method[0] # Choose the optimization method\n",
    "\n",
    "    X_train_GD = designMatrix_1D(x_train, best_poly_deg_GD) # Train design matrix for GD \n",
    "    X_test_GD = designMatrix_1D(x_test, best_poly_deg_GD) # Test design matrix for GD\n",
    "\n",
    "    X_train_SGD = designMatrix_1D(x_train, best_poly_deg_SGD) # Train design matrix for SGD\n",
    "    X_test_SGD = designMatrix_1D(x_test, best_poly_deg_SGD) # Test design matrix for SGD\n",
    "\n",
    "    y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "    y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Cost for the test data - SGD\") #zorder=0,\n",
    "    ax.plot(iters, test_cost_GD, color='crimson', lw=2, zorder=100, label=r\"Cost for the test data - GD\") #zorder=0,\n",
    "    # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for Adagrad w/ no momentum}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_Adagrad_w_o_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    ax.scatter(x_test, y_predict_SGD, c='limegreen', s=5, label='SGD')\n",
    "    ax.scatter(x_test, y_predict_GD, c='crimson', s=5,label='GD')\n",
    "    # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for Adagrad w/ no momentum}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_Adagrad_w_0_momentum.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "\"\"\" (8/8) cost-plot for either GD or SGD with Adagrad, Adam, RMSprop and with momentum using the chosen lambda \"\"\"\n",
    "def optim_plot_SGD(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.01                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.9                        # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    X_train_SGD = designMatrix_1D(x_train, best_poly_deg_SGD) # Train design matrix for SGD\n",
    "    X_test_SGD = designMatrix_1D(x_test, best_poly_deg_SGD) # Test design matrix for SGD\n",
    "\n",
    "\n",
    "    # Optimizer_method = ['Adagrad', 'RMSprop', 'Adam','momentum']\n",
    "    # colors_SGD = ['forestgreen', 'crimson', 'royalblue', 'darkorange']\n",
    "\n",
    "    Optimizer_method = ['RMSprop', 'Adagrad', 'Adam', 'momentum']\n",
    "    colors_SGD = ['crimson', 'forestgreen', 'royalblue', 'darkorange']\n",
    "    zor = [5, 10, 20, 50]\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "\n",
    "    for idx, O_M in enumerate(Optimizer_method):\n",
    "\n",
    "        y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "        ax.plot(iters, test_cost_SGD, c=colors_SGD[idx], zorder=zor[idx], lw=2, label=fr\"Cost - SGD using {O_M}\") #zorder=0,\n",
    "\n",
    "    # ax.plot(iters, test_cost_SGD, color='royalblue', lw=2, label=r\"Cost for the test data - SGD\") #zorder=0,\n",
    "    # plt.scatter(best_poly_deg_GD, np.min(MSE_test), color='forestgreen', marker='x', zorder=100, s=150, label='Lowest MSE')\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for different optimizers - SGD}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_optims_SGD.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    for idx, O_M in enumerate(Optimizer_method):\n",
    "\n",
    "        y_predict_SGD, theta, test_cost_SGD, train_cost_SGD = SGD(X_train_SGD, X_test_SGD, y_train, y_test, O_M, G_M, minibatch_size, n_minibatches, n_epochs, init_LR, decay, momentum, seed, best_lambda_SGD)\n",
    "\n",
    "        ax.scatter(x_test, y_predict_SGD, c=colors_SGD[idx], s=5, label=fr'SGD using {O_M}')\n",
    "\n",
    "    # ax.scatter(x_test, y_predict_OLS, c='dodgerblue', s=5, label='OLS')\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for different optimizers - SGD}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_optims_SGD.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "def optim_plot_GD(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, G_M, n_epochs, seed, best_minibatch_size):\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    init_LR = 0.1                          # Initial learning rate (LR)\n",
    "    decay = 0.01                             # init_LR/n_epochs # LR decay rate (for fixed LR set decay to 0)\n",
    "    momentum = 0.9                        # Momentum value for GD.\n",
    "    minibatch_size = np.shape(x_train)[0]//best_minibatch_size\n",
    "    n_minibatches = np.shape(x_train)[0]//minibatch_size #number of minibatches\n",
    "\n",
    "    X_train_GD = designMatrix_1D(x_train, best_poly_deg_GD) # Train design matrix for GD\n",
    "    X_test_GD = designMatrix_1D(x_test, best_poly_deg_GD) # Test design matrix for GD\n",
    "\n",
    "\n",
    "    Optimizer_method = ['RMSprop', 'Adagrad', 'Adam','momentum']\n",
    "    colors_GD = ['crimson', 'forestgreen', 'royalblue', 'darkorange']\n",
    "\n",
    "\n",
    "    iters = np.arange(n_epochs*n_minibatches)\n",
    "\n",
    "\n",
    "    plt.rc('axes', facecolor='whitesmoke', edgecolor='none',\n",
    "    axisbelow=True, grid=True)\n",
    "    plt.rc('grid', color='w', linestyle='solid')\n",
    "    plt.rc('lines', linewidth=2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "\n",
    "    for idx, O_M in enumerate(Optimizer_method):\n",
    "\n",
    "        y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "\n",
    "        ax.plot(iters, test_cost_GD, c=colors_GD[idx], lw=2, label=fr\"Cost - GD using {O_M}\") #zorder=0,\n",
    "\n",
    "    ax.set_xlabel(r\"Iterations\", labelpad=10)\n",
    "    ax.set_ylabel(r\"Cost\", labelpad=10)\n",
    "    ax.set_title(r\"\\bf{Cost as function of iterations for different optimizers - GD}\", pad=15)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('cost_plot_optims_GD.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "    \"\"\" Regression line plot \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    fig.subplots_adjust(bottom=0.22)\n",
    "    for idx, O_M in enumerate(Optimizer_method):\n",
    "\n",
    "        y_predict_GD, theta, test_cost_GD, train_cost_GD = GD(X_train_GD, X_test_GD, y_train, y_test, G_M, O_M, n_epochs*n_minibatches, init_LR, decay, momentum, seed, best_lambda_GD)\n",
    "\n",
    "        ax.scatter(x_test, y_predict_GD, c=colors_GD[idx], s=5, label=fr'GD using {O_M}')\n",
    "\n",
    "    ax.plot(x, y_true, zorder=100, c='black', label='True y')\n",
    "    ax.scatter(x_train, y_train, c='indigo', marker='o', s=3, alpha=0.3, label='Data') # Data\n",
    "    ax.set_title(r'\\bf{Regression line plot for different optimizers - GD}', pad=15)\n",
    "    ax.set_xlabel(r'$x$', labelpad=10)\n",
    "    ax.set_ylabel(r'$y$',  labelpad=10)\n",
    "    ax.legend(framealpha=0.9, facecolor=(1, 1, 1, 1))\n",
    "    string = f'init-LR = {init_LR}, decay = {decay}, momentum = {momentum}, n\\_epochs = {n_epochs}, minibatch\\_size = {minibatch_size}, n\\_minibatches = {n_minibatches}'\n",
    "    plt.figtext(0.5, 0.05, string, ha=\"center\", fontsize=18, bbox={'facecolor':'white', 'edgecolor':'black', 'lw':0.5, 'boxstyle':'round'})\n",
    "    plt.savefig('regression_line_plot_optims_GD.png', dpi=150)\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Data: \"\"\"\n",
    "coeff = [1.0, 1.0, 1.0]\n",
    "n = 1000 # Number of datapoints\n",
    "noise = True\n",
    "alpha = 0.1 # Noise scaling\n",
    "seed = 55 # Seed for noise (used to replicate results)\n",
    "x, y_noise, y_true = polynomial(coeff, n, noise, alpha, seed)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_noise, test_size=0.2, random_state = seed)\n",
    "\n",
    "\"\"\" Gradient Gradient_method \"\"\"\n",
    "Gradient_method = ['auto', 'anal']\n",
    "G_M = Gradient_method[1] #Choose the Gradient Gradient_method\n",
    "\n",
    "lambda_min = -15\n",
    "lambda_max = -1\n",
    "nlambdas = 15\n",
    "max_polydeg = 3\n",
    "\n",
    "plot = True\n",
    "n_epochs = 1000\n",
    "seed = 55\n",
    "\n",
    "\"\"\" Plots: \"\"\"\n",
    "best_poly_deg_GD, best_lambda_GD = find_lambda_DG(x_train, x_test, y_train, y_test, x, y_true, G_M, lambda_min, lambda_max, nlambdas, max_polydeg, plot, n_epochs, seed)\n",
    "best_poly_deg_SGD, best_lambda_SGD = find_lambda_SDG(x_train, x_test, y_train, y_test, x, y_true,G_M, lambda_min, lambda_max, nlambdas, max_polydeg, plot, n_epochs, seed)\n",
    "best_poly_deg_GD += 1\n",
    "best_poly_deg_SGD += 1\n",
    "\n",
    "best_minibatch_size = find_minibatch_size_SDG(x_train, x_test, y_train, y_test, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed)\n",
    "\n",
    "fixed_LR(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size)\n",
    "fixed_LR_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size)\n",
    "adaptive_LR_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size)\n",
    "Adagrad_w_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size)\n",
    "Adagrad_w_o_momentum(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size)\n",
    "optim_plot_SGD(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_SGD, best_lambda_SGD, G_M, n_epochs, seed, best_minibatch_size)\n",
    "optim_plot_GD(x_train, x_test, y_train, y_test, x, y_true, best_poly_deg_GD, best_lambda_GD, G_M, n_epochs, seed, best_minibatch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3de6e3",
   "metadata": {},
   "source": [
    "## <h2 style=\"text-align: center;\"> <ins> NB: </ins> </h2>\n",
    "\n",
    "I did not have enough time to elaborate on the theory nor aspects of analysis in this week`s set of exercises(sadly). This is mainly due to the work and process of finishing and delivering project 1, whilst in combination with tedious other mandatory assignments in other courses. I hope, nonetheless, that this is okay considering a lot work has already been put into project 2 code-wise (as seen above). \n",
    "\n",
    "I would also like to mention that I do not specifically know why this notebook fails in providing output when plotting (due to a fontsize issue), but I suspect it has to do with the version of matplotlib that is set as a standard (this was mainly written as a pure python file). \n",
    "\n",
    "Lastly, the autograd library is an exstension that can be pip installed on your computer. The link to where a more detailed documentation of it can be found is put here.\n",
    "\n",
    "Link: https://github.com/HIPS/autograd "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
